# evaluate_model.py
import pandas as pd
import numpy as np
import joblib
import requests
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score
import os
from datetime import datetime


def inspect_data(df, name="DataFrame"):
    """Inspecte les donn√©es pour debugger"""
    print(f"\nüîç Inspection {name}:")
    print(f"Colonnes: {df.columns.tolist()}")
    print(f"Forme: {df.shape}")
    if len(df) > 0:
        print("Aper√ßu:")
        print(df.head(2))


def evaluate_model():
    """√âvalue la performance du mod√®le"""
    print("üìä √âvaluation du mod√®le...")

    try:
        # Charger le mod√®le
        model_data = joblib.load("models/country_model.pkl")
        model = model_data['model']
        feature_columns = model_data['feature_columns']

        print(f"‚úÖ Mod√®le charg√©: {len(feature_columns)} features")
        print(f"üìã Features: {feature_columns}")

    except Exception as e:
        print(f"‚ùå Erreur chargement mod√®le: {e}")
        return None, None

    # Charger les donn√©es pour l'√©valuation
    try:
        response = requests.get("http://localhost:3001/api/stats/gdp-vs-medals", timeout=5)
        if response.status_code == 200:
            data = response.json()
            df = pd.DataFrame(data)
            print("‚úÖ Donn√©es API charg√©es")
        else:
            raise Exception("API non disponible")
    except Exception as e:
        print(f"‚ö†Ô∏è  API non disponible - {e}")
        print("üìù Utilisation donn√©es par d√©faut")
        df = create_default_data()

    inspect_data(df, "Donn√©es brutes")

    # Pr√©parer les donn√©es
    df_prepared = prepare_evaluation_data(df, feature_columns)

    if df_prepared.empty:
        print("‚ùå Pas de donn√©es pour l'√©valuation")
        return None, None

    inspect_data(df_prepared, "Donn√©es pr√©par√©es")

    # Features et cibles
    X = df_prepared[feature_columns]

    # Identifier les colonnes cibles
    target_columns = []
    possible_targets = [
        ['gold_count', 'silver_count', 'bronze_count'],
        ['hist_gold', 'hist_silver', 'hist_bronze'],
        ['Gold', 'Silver', 'Bronze']
    ]

    for target_set in possible_targets:
        if all(col in df_prepared.columns for col in target_set):
            target_columns = target_set
            break

    if not target_columns:
        print("‚ùå Colonnes cibles non trouv√©es")
        # Cr√©er des cibles simul√©es bas√©es sur les features
        df_prepared['simulated_gold'] = (df_prepared['hist_gold'] * 0.1).astype(int)
        df_prepared['simulated_silver'] = (df_prepared['hist_silver'] * 0.1).astype(int)
        df_prepared['simulated_bronze'] = (df_prepared['hist_bronze'] * 0.1).astype(int)
        target_columns = ['simulated_gold', 'simulated_silver', 'simulated_bronze']

    y_true = df_prepared[target_columns]

    print(f"üéØ Cibles utilis√©es: {target_columns}")

    # Pr√©dictions
    y_pred = model.predict(X)

    # M√©triques par type de m√©daille
    metrics = {}
    medal_types = ['Gold', 'Silver', 'Bronze']

    for i, medal_type in enumerate(medal_types):
        if i < len(target_columns):
            true_values = y_true.iloc[:, i]
            pred_values = y_pred[:, i]

            metrics[medal_type] = {
                'MAE': mean_absolute_error(true_values, pred_values),
                'MSE': mean_squared_error(true_values, pred_values),
                'R2': r2_score(true_values, pred_values),
                'RMSE': np.sqrt(mean_squared_error(true_values, pred_values)),
                'True_Mean': float(true_values.mean()),
                'Pred_Mean': float(pred_values.mean())
            }

    # M√©triques globales
    metrics['Global'] = {
        'MAE': mean_absolute_error(y_true, y_pred),
        'MSE': mean_squared_error(y_true, y_pred),
        'R2': r2_score(y_true, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred))
    }

    # Validation crois√©e
    try:
        cv_scores = cross_val_score(model, X, y_true, cv=min(5, len(X)), scoring='neg_mean_absolute_error')
        cv_mae = -cv_scores.mean()
        cv_std = cv_scores.std()
    except Exception as e:
        print(f"‚ö†Ô∏è  Validation crois√©e √©chou√©e: {e}")
        cv_mae = metrics['Global']['MAE']
        cv_std = 0

    # Taux de pr√©cision personnalis√©
    accuracy = calculate_custom_accuracy(y_true, y_pred)

    # Afficher les r√©sultats
    print("\n" + "=" * 60)
    print("üìà R√âSULTATS D'√âVALUATION DU MOD√àLE")
    print("=" * 60)

    print(f"\n‚úÖ TAUX DE PR√âCISION GLOBAL: {accuracy:.1%}")
    print(f"üîç Validation Crois√©e - MAE moyen: {cv_mae:.2f} ¬± {cv_std:.2f} m√©dailles")

    for medal_type, medal_metrics in metrics.items():
        if medal_type != 'Global':
            print(f"\nüéØ {medal_type}:")
            print(f"   üìè MAE: {medal_metrics['MAE']:.2f} m√©dailles")
            print(f"   üìê RMSE: {medal_metrics['RMSE']:.2f} m√©dailles")
            print(f"   üìä R¬≤: {medal_metrics['R2']:.3f}")
            print(f"   üìà Moyenne r√©elle: {medal_metrics['True_Mean']:.1f}")
            print(f"   üìà Moyenne pr√©dite: {medal_metrics['Pred_Mean']:.1f}")

    print(f"\nüåç Donn√©es utilis√©es: {len(X)} pays")
    print(f"üìã Features: {', '.join(feature_columns)}")

    # Interpr√©tation
    print(f"\nüí° INTERPR√âTATION:")
    if accuracy >= 0.8:
        print("   üéØ EXCELLENT - Le mod√®le est tr√®s pr√©cis")
    elif accuracy >= 0.6:
        print("   ‚úÖ BON - Le mod√®le est fiable")
    elif accuracy >= 0.4:
        print("   ‚ö†Ô∏è  CORRECT - Le mod√®le a besoin d'am√©liorations")
    else:
        print("   ‚ùå FAIBLE - Le mod√®le n√©cessite un r√©entra√Ænement")

    # Sauvegarder le rapport
    save_evaluation_report(metrics, cv_mae, accuracy, len(X), feature_columns)

    return metrics, accuracy


def calculate_custom_accuracy(y_true, y_pred):
    """Calcule un taux de pr√©cision personnalis√©"""
    accuracy_scores = []

    for i in range(len(y_true)):
        true_total = y_true.iloc[i].sum()
        pred_total = y_pred[i].sum()

        if true_total > 0:
            # Pourcentage d'erreur sur le total
            error = abs(pred_total - true_total) / true_total
            accuracy = max(0, 1 - error)
            accuracy_scores.append(accuracy)
        else:
            # Si pas de m√©dailles r√©elles, v√©rifier si la pr√©diction est basse
            if pred_total <= 5:  # Si pr√©dit peu de m√©dailles
                accuracy_scores.append(0.8)
            else:
                accuracy_scores.append(0.2)

    return np.mean(accuracy_scores) if accuracy_scores else 0


def prepare_evaluation_data(df, feature_columns):
    """Pr√©pare les donn√©es pour l'√©valuation"""
    df_clean = df.copy()

    # Inspecter les colonnes disponibles
    print(f"\nüìã Colonnes disponibles: {df_clean.columns.tolist()}")

    # Nettoyer GDP
    if 'gdp' in df_clean.columns:
        df_clean['gdp'] = pd.to_numeric(df_clean['gdp'], errors='coerce')
        df_clean = df_clean.dropna(subset=['gdp'])
    else:
        print("‚ö†Ô∏è  Colonne 'gdp' non trouv√©e")
        return pd.DataFrame()

    # Identifier les colonnes de m√©dailles historiques
    gold_col = None
    silver_col = None
    bronze_col = None

    possible_columns = {
        'gold': ['gold_count', 'hist_gold', 'Gold', 'gold_medals'],
        'silver': ['silver_count', 'hist_silver', 'Silver', 'silver_medals'],
        'bronze': ['bronze_count', 'hist_bronze', 'Bronze', 'bronze_medals']
    }

    for col_type, possible_names in possible_columns.items():
        for name in possible_names:
            if name in df_clean.columns:
                if col_type == 'gold':
                    gold_col = name
                elif col_type == 'silver':
                    silver_col = name
                elif col_type == 'bronze':
                    bronze_col = name
                break

    print(f"üéØ Colonnes m√©dailles trouv√©es: Gold={gold_col}, Silver={silver_col}, Bronze={bronze_col}")

    # Renommer les colonnes m√©dailles pour correspondre aux features du mod√®le
    rename_dict = {}
    if gold_col and gold_col != 'hist_gold':
        rename_dict[gold_col] = 'hist_gold'
    if silver_col and silver_col != 'hist_silver':
        rename_dict[silver_col] = 'hist_silver'
    if bronze_col and bronze_col != 'hist_bronze':
        rename_dict[bronze_col] = 'hist_bronze'

    if rename_dict:
        df_clean = df_clean.rename(columns=rename_dict)
        print(f"‚úÖ Colonnes renomm√©es: {rename_dict}")

    # Cr√©er les features manquantes
    if 'n_athletes' not in df_clean.columns:
        df_clean['n_athletes'] = (df_clean['hist_gold'] + df_clean['hist_silver'] + df_clean['hist_bronze']) * 1.5
        df_clean['n_athletes'] = df_clean['n_athletes'].astype(int).clip(10, 600)
        print("‚úÖ Feature n_athletes cr√©√©e")

    if 'n_sports' not in df_clean.columns:
        df_clean['n_sports'] = np.sqrt(df_clean['hist_gold'] + df_clean['hist_silver'] + df_clean['hist_bronze'])
        df_clean['n_sports'] = df_clean['n_sports'].astype(int).clip(5, 30)
        print("‚úÖ Feature n_sports cr√©√©e")

    if 'n_events' not in df_clean.columns:
        df_clean['n_events'] = (df_clean['hist_gold'] + df_clean['hist_silver'] + df_clean['hist_bronze']) * 1.2
        df_clean['n_events'] = df_clean['n_events'].astype(int).clip(10, 50)
        print("‚úÖ Feature n_events cr√©√©e")

    # V√©rifier que toutes les features sont pr√©sentes et dans le bon ordre
    missing_cols = set(feature_columns) - set(df_clean.columns)
    if missing_cols:
        print(f"‚ö†Ô∏è  Features manquantes: {missing_cols}")
        for col in missing_cols:
            df_clean[col] = 0

    # S'assurer que les colonnes sont dans le bon ordre
    final_columns = [col for col in feature_columns if col in df_clean.columns]
    missing_in_final = set(feature_columns) - set(final_columns)

    if missing_in_final:
        print(f"‚ùå Features manquantes dans l'ordre: {missing_in_final}")
        return pd.DataFrame()

    print(f"‚úÖ Features finales dans l'ordre: {final_columns}")

    # Retourner seulement les features dans le bon ordre + les cibles
    result_df = df_clean[final_columns + ['hist_gold', 'hist_silver', 'hist_bronze']]

    # Supprimer les doublons de colonnes
    result_df = result_df.loc[:, ~result_df.columns.duplicated()]

    print(f"‚úÖ Donn√©es pr√©par√©es: {result_df.shape}")
    return result_df


def create_default_data():
    """Cr√©e des donn√©es par d√©faut pour l'√©valuation"""
    return pd.DataFrame({
        'country_name': ['France', 'United States', 'China', 'Germany', 'Japan', 'Great Britain', 'Australia'],
        'gdp': [2.9e12, 23e12, 14e12, 4.0e12, 5.0e12, 3.0e12, 1.5e12],
        'gold_count': [250, 1000, 300, 400, 200, 300, 150],
        'silver_count': [280, 800, 250, 350, 150, 250, 120],
        'bronze_count': [300, 700, 200, 300, 100, 200, 100]
    })


def save_evaluation_report(metrics, cv_mae, accuracy, n_countries, feature_columns):
    """Sauvegarde un rapport d'√©valuation"""
    report = {
        'timestamp': datetime.now().isoformat(),
        'evaluation': {
            'n_countries': n_countries,
            'global_accuracy': accuracy,
            'cross_validation_mae': cv_mae,
            'feature_columns': feature_columns,
            'metrics': metrics
        },
        'interpretation': {
            'excellent': '> 80% - Mod√®le tr√®s pr√©cis',
            'good': '60% - 80% - Mod√®le fiable',
            'fair': '40% - 60% - Mod√®le acceptable',
            'poor': '< 40% - Mod√®le √† am√©liorer'
        }
    }

    os.makedirs("evaluation_reports", exist_ok=True)

    filename = f"evaluation_reports/report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(filename, 'w') as f:
        import json
        json.dump(report, f, indent=2)

    print(f"\nüíæ Rapport sauvegard√©: {filename}")


if __name__ == "__main__":
    metrics, accuracy = evaluate_model()

    if accuracy is not None:
        print(f"\nüéâ √âvaluation termin√©e avec succ√®s!")
        print(f"üìä Pr√©cision du mod√®le: {accuracy:.1%}")
    else:
        print("\n‚ùå √âvaluation √©chou√©e")